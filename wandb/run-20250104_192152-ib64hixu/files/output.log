/home/parkchan/.conda/envs/facegraph/lib/python3.9/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Train Epoch 1:   0%|                                                    | 0/900 [00:57<?, ?it/s]
Traceback (most recent call last):
  File "/home/parkchan/face_roi_graph/src/train.py", line 91, in <module>
    main()
  File "/home/parkchan/face_roi_graph/src/train.py", line 88, in main
    main_train(config)
  File "/home/parkchan/face_roi_graph/src/train.py", line 61, in main_train
    train_loss, train_acc, train_auc = train_one_epoch(
  File "/home/parkchan/face_roi_graph/src/engine/trainer.py", line 24, in train_one_epoch
    frames = batch["frames"].to(device)   # (B,32,C,H,W)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 74.00 MiB. GPU 0 has a total capacity of 23.69 GiB of which 9.75 MiB is free. Process 11073 has 15.22 GiB memory in use. Process 34435 has 1.12 GiB memory in use. Process 34433 has 1.51 GiB memory in use. Process 34434 has 1.04 GiB memory in use. Process 34430 has 1.50 GiB memory in use. Including non-PyTorch memory, this process has 819.00 MiB memory in use. Process 34431 has 903.00 MiB memory in use. Process 34429 has 819.00 MiB memory in use. Process 34436 has 819.00 MiB memory in use. Of the allocated memory 46.97 MiB is allocated by PyTorch, and 15.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
